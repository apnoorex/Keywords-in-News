{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords in News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to search the news articles on given websites for keywords and perform subsequent statistical analysis. The result are presented in a form of a Power BI report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following keywords are investigated: Elections, Black Friday, and NFL. The time period considered is November 1-15 of 2024. The task is to see how often the keywords are being mentioned in the articles title and description.\n",
    "\n",
    "The information we are going to collect:\n",
    "- Total number of times the word was mentioned,\n",
    "- Number of times the word was mentioned each day,\n",
    "- The day when the keyword was mentioned the most number of times,\n",
    "- Three newspapers where the keywords is seen the most,\n",
    "- How often the keyword is mentioned comparing to other popular topics.\n",
    "\n",
    "The access to the article storage is provided by NEWS Api, where an api_id was obtained beforehand. Python is used to perform the analysis.\n",
    "\n",
    "The results are presented in Power BI. NLTK Tokenizer is used for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will discuss the code that is used to collect the data. The idea is to loop through all the dates, collect the data for each day separately, store that data, and then create a final dataset that will contain all of the information we need for the analysis.\n",
    "\n",
    "We have to start with importing Python libraries that will be needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access to the articles database, we need to create an object of class NewsApiClient using the api_id that we have. The **.get_everything()** method is then used to download the data according to the parameters we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key='your_api_id')\n",
    "\n",
    "all_articles = newsapi.get_everything(domains=domains,\n",
    "                                    from_param=ddate,\n",
    "                                    to=ddate,\n",
    "                                    language='en',\n",
    "                                    sort_by='relevancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using **datetime** Python library to help us build the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime(year, month, day)\n",
    "date += datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we a going to use NLTK library to tokenize the incoming data. The functionality of this library allows us only keep the nouns clearing out the rest of text, and also take care of things like plural and singular forms of a noun and other similar things. Some custom code will also be added to go along to handle situations when the same word appears with a capital and lower case letter in different articles. Also, we will use a dictionary of stop words that we don't want in our analysis. It's mainly ment to store some special characters and prepositions that were missed by NLTK. In addition, it will contain first names of popular public people to prevent counting the same person twice. For example, we will be skipping words Elon and Donald. The stop words can be found in *stop_dict.py* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(whole_text)\n",
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set to get started.\n",
    "\n",
    "So, in the first part of the project we are going to store top mentioned words per day for November 1-15 of 2024 with the number of times it was mentioned. We defiine top mentioned words as the ones that we see 4 times or more that day on the news websites that we examine. The list of that websites is stored in the *domains.py* file.\n",
    "\n",
    "The second piece of information we will be looking for is the number of time our three keywords - Elections, Black Friday, and NFL - appear on the news each day, independently of if they are on the list of top mentioned words or not.\n",
    "\n",
    "Note: To investigate the keyword Black Friday, we will be using only the word Friday, putting the word Black on the stop list. Like we decided to do for the famous people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete code for this part of the project looks like this. Only one keyword is used in the code below. That is done to simplify the code for demonstration purposes. More keywords can be added in the same manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of news domains\n",
    "from domains import domains\n",
    "# We will use an additional list of stop words\n",
    "from stop_dict import stop_dict\n",
    "\n",
    "# Save results to file\n",
    "def save_key_stats_to_file(filename, stats):\n",
    "    d = datetime.datetime(year, month, day)\n",
    "    with open(filename, 'w') as f:\n",
    "        for line in stats:\n",
    "            f.write(f\"{str(d).split()[0]},{line}\\n\")\n",
    "            d += datetime.timedelta(days=1)\n",
    "\n",
    "# Parameters\n",
    "year = 2024\n",
    "month = 11\n",
    "day = 1\n",
    "n_days = 15\n",
    "keyword_1 = 'Election'\n",
    "\n",
    "date = datetime.datetime(year, month, day)\n",
    "# Init\n",
    "newsapi = NewsApiClient(api_key='e3a92ef3e7664dc1be44f76fa900828f')\n",
    "\n",
    "keyword_1_stats = []\n",
    "for _ in range(n_days):\n",
    "    ddate = str(date).split()[0]\n",
    "\n",
    "    all_articles = newsapi.get_everything(domains=domains,\n",
    "                                        from_param=ddate,\n",
    "                                        to=ddate,\n",
    "                                        language='en',\n",
    "                                        sort_by='relevancy')\n",
    "\n",
    "    whole_text = ''\n",
    "    for article in all_articles['articles']:\n",
    "        whole_text += article['title']\n",
    "        whole_text += str(article['description'])\n",
    "\n",
    "    tokens = nltk.word_tokenize(whole_text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    clean = []\n",
    "    for tag in tagged:\n",
    "        if tag[1] in ['NNP', 'NN']:\n",
    "            w = tag[0].title() if tag[0][0].islower() else tag[0]\n",
    "            if w not in stop_dict:\n",
    "                clean.append(w)\n",
    "                \n",
    "    most_common = Counter(clean)\n",
    "\n",
    "    keywords = {}\n",
    "    total_n_keywords = 0\n",
    "    for mc in most_common.items():\n",
    "        if mc[1] > 3:\n",
    "            keywords[mc[0]] = mc[1]\n",
    "            total_n_keywords += 1\n",
    "        if mc[0] == keyword_1:\n",
    "            keyword_1_stats.append(mc[1])\n",
    "\n",
    "    if keyword_1 not in most_common.keys():\n",
    "        keyword_1_stats.append(0)\n",
    "\n",
    "    # Save data to a CSV file\n",
    "    with open(f\"top_keywords_for_{ddate}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for row in keywords.items():\n",
    "            writer.writerow(row)\n",
    "\n",
    "    date += datetime.timedelta(days=1)\n",
    "\n",
    "save_key_stats_to_file(f'{keyword_1}_stats.csv', keyword_1_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the project we will be looking for the three sources where the keywords appear the most.\n",
    "\n",
    "Here is the code that is used to acomplish that. The names of the three sources are being save to a file separately for each keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['Election', 'Friday', 'NFL']\n",
    "\n",
    "for keyword in keywords:\n",
    "    all_articles = newsapi.get_everything(q=keyword,\n",
    "                                        domains=domains,\n",
    "                                        from_param=date,\n",
    "                                        to=date,\n",
    "                                        language='en',\n",
    "                                        sort_by='relevancy')\n",
    "\n",
    "    papers = []\n",
    "    for headline in all_articles['articles']:\n",
    "        papers.append(headline['source']['name'])\n",
    "\n",
    "    most_common = Counter(papers)\n",
    "    most_common = list(most_common.keys())\n",
    "    most_common_sources = most_common[:3]\n",
    "\n",
    "    with open(f'{keyword}_sources.txt', 'w') as f:\n",
    "        for line in most_common_sources:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is to assemble the final table of all the data that we need for the analysis. Simply enough, we a going to loop through the files with the most popular words for each day and calculate the sum of all the mentions of all the top words. That will be the first column for the final table that we need. The second is the number of time the keyword appears on the news that day. And finally, the number of time the keyword appears on the news if it was among the most popular words that day having zero value otherwise.\n",
    "\n",
    "Here is the code to perform that. The final table is stored as a CSV file separately for each keyword. **Pandas** library will help us do all of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "total_counts = []\n",
    "keyword = 'Election'\n",
    "keyword_stats_if_popular = []\n",
    "\n",
    "for idx in range(1, 16, 1):\n",
    "    d = str(idx) if idx > 9 else '0' + str(idx)\n",
    "\n",
    "    keywords = {}\n",
    "    keywords_counts = []\n",
    "    with open(f\"top_keywords_for_2024-11-{d}.csv\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            keywords[line.split(',')[0]] = line.split(',')[1]\n",
    "            keywords_counts.append(int(line.split(',')[1]))\n",
    "    total_counts.append(sum(keywords_counts))\n",
    "\n",
    "    if keyword_1 in keywords.keys():\n",
    "        keyword_stats_if_popular.append(int(keywords[keyword_1]))\n",
    "    else:\n",
    "        keyword_stats_if_popular.append(0)\n",
    "\n",
    "df = pd.read_csv('Election_stats.csv', names=['Date', 'KeyWCount'])\n",
    "df['KeyWCountIfPop'] = keyword_stats_if_popular\n",
    "df['Totals'] = total_counts\n",
    "df.to_csv('Election_final_stats.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all of the data, we are ready to plot some graphs and see if the data we have collected contains any interesting and remarkable patterns.\n",
    "\n",
    "Power BI is used to plot the graph we need. PDF with the report for each keyword can be found in the root directory for this project.\n",
    "\n",
    "Let's see if we can derive any conclusions looking at those reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the word Elections can be seen the most on November 5 and 6 - the Election Day in the United States and the day after. It holds about 15% of all the mentions of top words on the news that day.\n",
    "\n",
    "After that the use of this word decreases drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Black Friday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Black Friday keyword we can observe that the popularity of this words grows as the date approaches. \n",
    "\n",
    "Also, its usage peaks on Saturdays, which is explainable as the companies intensify the advertising during the weekends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NFL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NFL keyword almost never appears on the news on working days. However, it takes a substantial amount of attention on Sundays, taking up to more that 7% of all the hot topics on November 9, for instance.\n",
    "\n",
    "We can also notice that ESPN and CNET are the top sources where NFL is discussed. Which is, of course, natural."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
